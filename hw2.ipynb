{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Laguage Inference (NLI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "import random\n",
    "import pdb\n",
    "import io\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import pandas as pd\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stanford NLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 50000\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Fasttext vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(f):\n",
    "    fin = io.open(f, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    data={}\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    for i,line in enumerate(fin):\n",
    "        if i == VOCAB_SIZE:\n",
    "            break\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = np.array(list(map(float, tokens[1:])))\n",
    "        i += 1\n",
    "    return data\n",
    "\n",
    "word_vectors = load_vectors(\"/Users/zihaoguo/Documents/nyu/ds1011/hw2/wiki-news-300d-1M.vec\")\n",
    "\n",
    "with open('fasttext_word_vectors.p', 'wb') as f:\n",
    "    pkl.dump(word_vectors, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dictionary, id2token, token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab():\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    word_vectors = pkl.load(open(\"fasttext_word_vectors.p\", \"rb\"))\n",
    "    id2token = list(word_vectors.keys())\n",
    "    token2id = dict(zip(word_vectors, range(2,2+len(word_vectors)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return word_vectors, token2id, id2token\n",
    "\n",
    "word_vectors, token2id, id2token = build_vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "snli_train = pd.read_csv('/Users/zihaoguo/Documents/nyu/ds1011/hw2/snli_train.tsv', sep='\\t')\n",
    "sent1_train = list(snli_train[\"sentence1\"])\n",
    "sent2_train = list(snli_train[\"sentence2\"])\n",
    "train_label = list(snli_train[\"label\"])\n",
    "print(len(train_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to integers\n",
    "def labels_to_integers(data_label):\n",
    "    for i in range(len(data_label)):\n",
    "        if data_label[i] == \"contradiction\":\n",
    "            data_label[i] = 0\n",
    "        elif data_label[i] == \"entailment\":\n",
    "            data_label[i] = 1\n",
    "        elif data_label[i] == \"neutral\":\n",
    "            data_label[i] = 2\n",
    "    return data_label\n",
    "\n",
    "train_label = labels_to_integers(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly shuffle the data so that we'll split them in the next step\n",
    "SEED = 123\n",
    "random.Random(SEED).shuffle(sent1_train)\n",
    "random.Random(SEED).shuffle(sent2_train)\n",
    "random.Random(SEED).shuffle(train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "snli_val = pd.read_csv('snli_val.tsv', sep='\\t')\n",
    "sent1_val = list(snli_val[\"sentence1\"])\n",
    "sent2_val = list(snli_val[\"sentence2\"])\n",
    "val_label = list(snli_val[\"label\"])\n",
    "val_label = labels_to_integers(val_label)\n",
    "SEED = 123\n",
    "random.Random(SEED).shuffle(sent1_val)\n",
    "random.Random(SEED).shuffle(sent2_val)\n",
    "random.Random(SEED).shuffle(val_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence_list):\n",
    "    return [word_tokenize(sentence_list[i]) for i in range(len(sentence_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "sent1_train_tokenized = tokenize(sent1_train)\n",
    "sent2_train_tokenized = tokenize(sent2_train)\n",
    "\n",
    "# val\n",
    "sent1_val_tokenized = tokenize(sent1_val)\n",
    "sent2_val_tokenized = tokenize(sent2_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot code the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "sent1_train_indices = token2index_dataset(sent1_train_tokenized)\n",
    "sent2_train_indices = token2index_dataset(sent2_train_tokenized)\n",
    "\n",
    "# val\n",
    "sent1_val_indices = token2index_dataset(sent1_val_tokenized)\n",
    "sent2_val_indices = token2index_dataset(sent2_val_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Pytorch Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 80\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NLIDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sent1_data_list, sent2_data_list, target_list):\n",
    "        self.sent1_data_list = sent1_data_list\n",
    "        self.sent2_data_list = sent2_data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.sent1_data_list) == len(self.target_list) and len(self.sent2_data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sent1_data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        ###\n",
    "        ### Returns [[sentence, 1, tokens], [sentence, 2, tokens]]\n",
    "        ###\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        sent1_tokens_idx = self.sent1_data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        sent2_tokens_idx = self.sent2_data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        combined_tokens_idx = [sent1_tokens_idx, sent2_tokens_idx]\n",
    "        label = self.target_list[key]\n",
    "        return [combined_tokens_idx, len(sent1_tokens_idx), len(sent2_tokens_idx), label]\n",
    "\n",
    "def NLI_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    sent1_data_list = []\n",
    "    sent2_data_list = []\n",
    "    sent1_length_list = []\n",
    "    sent2_length_list = []\n",
    "    label_list = []\n",
    "    combined_data_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[3])\n",
    "        sent1_length_list.append(datum[1])\n",
    "        sent2_length_list.append(datum[2])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec_1 = np.pad(np.array(datum[0][0]), pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        padded_vec_2 = np.pad(np.array(datum[0][1]), pad_width=((0,MAX_SENTENCE_LENGTH-datum[2])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        combined_data_list.append([padded_vec_1, padded_vec_2])\n",
    "    return [torch.from_numpy(np.array(combined_data_list)), \n",
    "            torch.LongTensor(sent1_length_list), torch.LongTensor(sent2_length_list), torch.LongTensor(label_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NLIDataset(sent1_train_indices, sent2_train_indices, train_label)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE, \n",
    "                                           collate_fn=NLI_collate_func,\n",
    "                                           shuffle=True\n",
    "                                          )\n",
    "\n",
    "val_dataset = NLIDataset(sent1_val_indices, sent2_val_indices, val_label)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE, \n",
    "                                           collate_fn=NLI_collate_func,\n",
    "                                           shuffle=True\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.array(list(word_vectors.values()))\n",
    "pad_vec = np.zeros((1, 300))\n",
    "unk_vec = np.random.randn(1, 300) * 0.01\n",
    "pad_unk_vecs = np.vstack((pad_vec, unk_vec))\n",
    "WEIGHTS = np.vstack((pad_unk_vecs, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 300)"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rnn(nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers, num_classes, emb_size= 300):\n",
    "        # RNN Accepts the following hyperparams:\n",
    "        # emb_size: Embedding Size\n",
    "        # hidden_size: Hidden Size of layer in RNN\n",
    "        # num_layers: number of layers in RNN\n",
    "        # num_classes: number of output classes\n",
    "        # vocab_size: vocabulary size\n",
    "        super(rnn, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        weight = torch.FloatTensor(WEIGHTS)\n",
    "        self.embedding = nn.Embedding.from_pretrained(weight)\n",
    "        self.rnn = nn.GRU(emb_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.linear1 = nn.Linear(2*hidden_size, 500)\n",
    "        self.linear2 = nn.Linear(500, num_classes)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Function initializes the activation of recurrent neural net at timestep 0\n",
    "        # Needs to be in format (num_layers, batch_size, hidden_size)\n",
    "        return torch.randn(2, batch_size*2, self.hidden_size)\n",
    "\n",
    "    def forward(self, x, sent1_lengths, sent2_lengths):\n",
    "        # reset hidden state\n",
    "        batch_size = x.size()[0]\n",
    "                \n",
    "        s1lengths = list(sent1_lengths)\n",
    "        s2lengths = list(sent2_lengths)\n",
    "        ordered_slengths = s1lengths + s2lengths\n",
    "\n",
    "        reverse_sorted_indices = [x for _, x in sorted(zip(ordered_slengths, range(len(ordered_slengths))), reverse=True)]\n",
    "        reverse_sorted_lengths = [x for x, _ in sorted(zip(ordered_slengths, range(len(ordered_slengths))), reverse=True)]\n",
    "        reverse_sorted_lengths = np.array(reverse_sorted_lengths)\n",
    "        \n",
    "        sent1s = x[:, 0, :]\n",
    "        sent2s = x[:, 1, :]\n",
    "        ordered_sents = torch.cat([sent1s, sent2s], dim=0)\n",
    "        reverse_sorted_data = torch.index_select(ordered_sents, 0, torch.tensor(reverse_sorted_indices))\n",
    "        \n",
    "        # get embedding\n",
    "        embed = self.embedding(reverse_sorted_data) \n",
    "        self.hidden = self.init_hidden(batch_size)\n",
    "        # pack padded sequence\n",
    "        embed = torch.nn.utils.rnn.pack_padded_sequence(embed, reverse_sorted_lengths, batch_first=True)       \n",
    "        # fprop though RNN\n",
    "        rnn_out, self.hidden = self.rnn(embed, self.hidden)        \n",
    "        ### MATCHING BACK\n",
    "        change_it_back = [x for _, x in sorted(zip(reverse_sorted_indices, range(len(reverse_sorted_indices))))]\n",
    "        self.hidden = torch.index_select(self.hidden, 1, torch.LongTensor(change_it_back)) \n",
    "        ### GRU\n",
    "        hidden_sent1s = torch.cat([self.hidden[0, 0:batch_size, :], self.hidden[1, 0:batch_size, :]], dim=1)\n",
    "        hidden_sent2s = torch.cat([self.hidden[0, batch_size:, :], self.hidden[1, batch_size:, :]], dim=1)\n",
    "        \n",
    "#         concatenation of encoded sentences\n",
    "#        linear1 = self.linear1(torch.cat([hidden_sent1s, hidden_sent2s], dim=1))\n",
    "#         addition of encoded sentences\n",
    "#         linear1 = self.linear1(torch.tensor(hidden_sent1s) + torch.tensor(hidden_sent2s))\n",
    "#         element-wise multiplication of encoded sentences\n",
    "        linear1 = self.linear1(torch.tensor(hidden_sent1s)*torch.tensor(hidden_sent2s))\n",
    "        linear1 = F.relu(linear1.contiguous().view(-1, linear1.size(-1))).view(linear1.shape)   \n",
    "#         linear1 = self.dropout(linear1)\n",
    "        logits = self.linear2(linear1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Helper function that tests the model's performance on a dataset\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for (data, sent1_lengths, sent2_lengths, labels) in loader:\n",
    "        data_batch, sent1_length_batch, sent2_length_batch, label_batch = data, sent1_lengths, sent2_lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, sent1_length_batch, sent2_length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        labels = labels\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "def train_model(model, lr = 0.001, num_epochs = 5, criterion = nn.CrossEntropyLoss()):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr) \n",
    "    max_val_acc = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (data, sent1_lengths, sent2_lengths, labels) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            data_batch, sent1_length_batch, sent2_length_batch, label_batch = data, sent1_lengths, sent2_lengths, labels\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data_batch, sent1_length_batch, sent2_length_batch)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # validate every 100 iterations\n",
    "            if i > 0 and i % 100 == 0:\n",
    "                # validate\n",
    "                val_acc = test_model(val_loader, model)\n",
    "                if val_acc > max_val_acc:\n",
    "                    max_val_acc = val_acc\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                           epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Training Loss: {}'.format( \n",
    "                           epoch+1, num_epochs, i+1, len(train_loader), loss))\n",
    "                \n",
    "    print(\"Max Validation Accuracy: {}\".format(max_val_acc))\n",
    "    return max_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/5], Step: [101/3125], Validation Acc: 36.5\n",
      "Epoch: [1/5], Step: [101/3125], Training Loss: 1.1044721603393555\n",
      "Epoch: [1/5], Step: [201/3125], Validation Acc: 40.9\n",
      "Epoch: [1/5], Step: [201/3125], Training Loss: 1.0390400886535645\n",
      "Epoch: [1/5], Step: [301/3125], Validation Acc: 46.5\n",
      "Epoch: [1/5], Step: [301/3125], Training Loss: 0.9962356686592102\n",
      "Epoch: [1/5], Step: [401/3125], Validation Acc: 46.3\n",
      "Epoch: [1/5], Step: [401/3125], Training Loss: 1.012796401977539\n",
      "Epoch: [1/5], Step: [501/3125], Validation Acc: 52.8\n",
      "Epoch: [1/5], Step: [501/3125], Training Loss: 0.9181193113327026\n",
      "Epoch: [1/5], Step: [601/3125], Validation Acc: 55.1\n",
      "Epoch: [1/5], Step: [601/3125], Training Loss: 0.8689990639686584\n",
      "Epoch: [1/5], Step: [701/3125], Validation Acc: 54.6\n",
      "Epoch: [1/5], Step: [701/3125], Training Loss: 0.8240164518356323\n",
      "Epoch: [1/5], Step: [801/3125], Validation Acc: 56.3\n",
      "Epoch: [1/5], Step: [801/3125], Training Loss: 0.8364970088005066\n",
      "Epoch: [1/5], Step: [901/3125], Validation Acc: 57.0\n",
      "Epoch: [1/5], Step: [901/3125], Training Loss: 0.9271654486656189\n",
      "Epoch: [1/5], Step: [1001/3125], Validation Acc: 52.8\n",
      "Epoch: [1/5], Step: [1001/3125], Training Loss: 0.9419674277305603\n",
      "Epoch: [1/5], Step: [1101/3125], Validation Acc: 59.0\n",
      "Epoch: [1/5], Step: [1101/3125], Training Loss: 0.7228213548660278\n",
      "Epoch: [1/5], Step: [1201/3125], Validation Acc: 59.8\n",
      "Epoch: [1/5], Step: [1201/3125], Training Loss: 0.7221224308013916\n",
      "Epoch: [1/5], Step: [1301/3125], Validation Acc: 61.5\n",
      "Epoch: [1/5], Step: [1301/3125], Training Loss: 0.9322333931922913\n",
      "Epoch: [1/5], Step: [1401/3125], Validation Acc: 61.9\n",
      "Epoch: [1/5], Step: [1401/3125], Training Loss: 0.9708647727966309\n",
      "Epoch: [1/5], Step: [1501/3125], Validation Acc: 62.0\n",
      "Epoch: [1/5], Step: [1501/3125], Training Loss: 0.848944365978241\n",
      "Epoch: [1/5], Step: [1601/3125], Validation Acc: 62.1\n",
      "Epoch: [1/5], Step: [1601/3125], Training Loss: 0.758427619934082\n",
      "Epoch: [1/5], Step: [1701/3125], Validation Acc: 62.6\n",
      "Epoch: [1/5], Step: [1701/3125], Training Loss: 0.996613085269928\n",
      "Epoch: [1/5], Step: [1801/3125], Validation Acc: 63.1\n",
      "Epoch: [1/5], Step: [1801/3125], Training Loss: 0.8158227205276489\n",
      "Epoch: [1/5], Step: [1901/3125], Validation Acc: 64.3\n",
      "Epoch: [1/5], Step: [1901/3125], Training Loss: 0.7925201654434204\n",
      "Epoch: [1/5], Step: [2001/3125], Validation Acc: 64.0\n",
      "Epoch: [1/5], Step: [2001/3125], Training Loss: 0.9679181575775146\n",
      "Epoch: [1/5], Step: [2101/3125], Validation Acc: 65.4\n",
      "Epoch: [1/5], Step: [2101/3125], Training Loss: 0.8935292363166809\n",
      "Epoch: [1/5], Step: [2201/3125], Validation Acc: 62.8\n",
      "Epoch: [1/5], Step: [2201/3125], Training Loss: 0.9398524761199951\n",
      "Epoch: [1/5], Step: [2301/3125], Validation Acc: 65.1\n",
      "Epoch: [1/5], Step: [2301/3125], Training Loss: 0.6169103980064392\n",
      "Epoch: [1/5], Step: [2401/3125], Validation Acc: 65.4\n",
      "Epoch: [1/5], Step: [2401/3125], Training Loss: 0.6693323850631714\n",
      "Epoch: [1/5], Step: [2501/3125], Validation Acc: 64.9\n",
      "Epoch: [1/5], Step: [2501/3125], Training Loss: 0.6416987180709839\n",
      "Epoch: [1/5], Step: [2601/3125], Validation Acc: 65.0\n",
      "Epoch: [1/5], Step: [2601/3125], Training Loss: 0.9074881672859192\n",
      "Epoch: [1/5], Step: [2701/3125], Validation Acc: 63.9\n",
      "Epoch: [1/5], Step: [2701/3125], Training Loss: 0.910256564617157\n",
      "Epoch: [1/5], Step: [2801/3125], Validation Acc: 66.6\n",
      "Epoch: [1/5], Step: [2801/3125], Training Loss: 0.9147042036056519\n",
      "Epoch: [1/5], Step: [2901/3125], Validation Acc: 67.3\n",
      "Epoch: [1/5], Step: [2901/3125], Training Loss: 0.6814634203910828\n",
      "Epoch: [1/5], Step: [3001/3125], Validation Acc: 65.5\n",
      "Epoch: [1/5], Step: [3001/3125], Training Loss: 0.5058731436729431\n",
      "Epoch: [1/5], Step: [3101/3125], Validation Acc: 65.1\n",
      "Epoch: [1/5], Step: [3101/3125], Training Loss: 0.665081262588501\n",
      "Epoch: [2/5], Step: [101/3125], Validation Acc: 66.6\n",
      "Epoch: [2/5], Step: [101/3125], Training Loss: 0.6731014847755432\n",
      "Epoch: [2/5], Step: [201/3125], Validation Acc: 66.8\n",
      "Epoch: [2/5], Step: [201/3125], Training Loss: 0.7078389525413513\n",
      "Epoch: [2/5], Step: [301/3125], Validation Acc: 65.5\n",
      "Epoch: [2/5], Step: [301/3125], Training Loss: 0.9081358909606934\n",
      "Epoch: [2/5], Step: [401/3125], Validation Acc: 64.5\n",
      "Epoch: [2/5], Step: [401/3125], Training Loss: 0.8213855028152466\n",
      "Epoch: [2/5], Step: [501/3125], Validation Acc: 67.1\n",
      "Epoch: [2/5], Step: [501/3125], Training Loss: 0.7057418823242188\n",
      "Epoch: [2/5], Step: [601/3125], Validation Acc: 67.7\n",
      "Epoch: [2/5], Step: [601/3125], Training Loss: 0.7248096466064453\n",
      "Epoch: [2/5], Step: [701/3125], Validation Acc: 69.4\n",
      "Epoch: [2/5], Step: [701/3125], Training Loss: 0.6046246290206909\n",
      "Epoch: [2/5], Step: [801/3125], Validation Acc: 66.9\n",
      "Epoch: [2/5], Step: [801/3125], Training Loss: 0.7812638878822327\n",
      "Epoch: [2/5], Step: [901/3125], Validation Acc: 66.5\n",
      "Epoch: [2/5], Step: [901/3125], Training Loss: 0.77508544921875\n",
      "Epoch: [2/5], Step: [1001/3125], Validation Acc: 67.4\n",
      "Epoch: [2/5], Step: [1001/3125], Training Loss: 0.8515840768814087\n",
      "Epoch: [2/5], Step: [1101/3125], Validation Acc: 67.0\n",
      "Epoch: [2/5], Step: [1101/3125], Training Loss: 0.7653946280479431\n",
      "Epoch: [2/5], Step: [1201/3125], Validation Acc: 67.5\n",
      "Epoch: [2/5], Step: [1201/3125], Training Loss: 0.8274113535881042\n",
      "Epoch: [2/5], Step: [1301/3125], Validation Acc: 67.9\n",
      "Epoch: [2/5], Step: [1301/3125], Training Loss: 0.5435558557510376\n",
      "Epoch: [2/5], Step: [1401/3125], Validation Acc: 69.7\n",
      "Epoch: [2/5], Step: [1401/3125], Training Loss: 0.872085452079773\n",
      "Epoch: [2/5], Step: [1501/3125], Validation Acc: 67.1\n",
      "Epoch: [2/5], Step: [1501/3125], Training Loss: 0.7017318606376648\n",
      "Epoch: [2/5], Step: [1601/3125], Validation Acc: 67.1\n",
      "Epoch: [2/5], Step: [1601/3125], Training Loss: 0.9368082880973816\n",
      "Epoch: [2/5], Step: [1701/3125], Validation Acc: 68.9\n",
      "Epoch: [2/5], Step: [1701/3125], Training Loss: 0.5675997138023376\n",
      "Epoch: [2/5], Step: [1801/3125], Validation Acc: 70.8\n",
      "Epoch: [2/5], Step: [1801/3125], Training Loss: 0.6432134509086609\n",
      "Epoch: [2/5], Step: [1901/3125], Validation Acc: 67.9\n",
      "Epoch: [2/5], Step: [1901/3125], Training Loss: 0.7225494384765625\n",
      "Epoch: [2/5], Step: [2001/3125], Validation Acc: 68.3\n",
      "Epoch: [2/5], Step: [2001/3125], Training Loss: 0.7001729607582092\n",
      "Epoch: [2/5], Step: [2101/3125], Validation Acc: 68.4\n",
      "Epoch: [2/5], Step: [2101/3125], Training Loss: 0.4448261857032776\n",
      "Epoch: [2/5], Step: [2201/3125], Validation Acc: 68.4\n",
      "Epoch: [2/5], Step: [2201/3125], Training Loss: 0.5603277683258057\n",
      "Epoch: [2/5], Step: [2301/3125], Validation Acc: 69.2\n",
      "Epoch: [2/5], Step: [2301/3125], Training Loss: 0.8118323087692261\n",
      "Epoch: [2/5], Step: [2401/3125], Validation Acc: 68.4\n",
      "Epoch: [2/5], Step: [2401/3125], Training Loss: 1.0881708860397339\n",
      "Epoch: [2/5], Step: [2501/3125], Validation Acc: 67.5\n",
      "Epoch: [2/5], Step: [2501/3125], Training Loss: 0.5302656292915344\n",
      "Epoch: [2/5], Step: [2601/3125], Validation Acc: 69.3\n",
      "Epoch: [2/5], Step: [2601/3125], Training Loss: 1.1892515420913696\n",
      "Epoch: [2/5], Step: [2701/3125], Validation Acc: 69.0\n",
      "Epoch: [2/5], Step: [2701/3125], Training Loss: 0.5324150919914246\n",
      "Epoch: [2/5], Step: [2801/3125], Validation Acc: 66.6\n",
      "Epoch: [2/5], Step: [2801/3125], Training Loss: 0.8336604237556458\n",
      "Epoch: [2/5], Step: [2901/3125], Validation Acc: 69.4\n",
      "Epoch: [2/5], Step: [2901/3125], Training Loss: 0.45499810576438904\n",
      "Epoch: [2/5], Step: [3001/3125], Validation Acc: 68.5\n",
      "Epoch: [2/5], Step: [3001/3125], Training Loss: 0.5602049827575684\n",
      "Epoch: [2/5], Step: [3101/3125], Validation Acc: 67.1\n",
      "Epoch: [2/5], Step: [3101/3125], Training Loss: 0.7792303562164307\n",
      "Epoch: [3/5], Step: [101/3125], Validation Acc: 67.7\n",
      "Epoch: [3/5], Step: [101/3125], Training Loss: 0.4371456503868103\n",
      "Epoch: [3/5], Step: [201/3125], Validation Acc: 68.6\n",
      "Epoch: [3/5], Step: [201/3125], Training Loss: 0.4169059991836548\n",
      "Epoch: [3/5], Step: [301/3125], Validation Acc: 68.8\n",
      "Epoch: [3/5], Step: [301/3125], Training Loss: 0.8015825152397156\n",
      "Epoch: [3/5], Step: [401/3125], Validation Acc: 70.3\n",
      "Epoch: [3/5], Step: [401/3125], Training Loss: 0.6622280478477478\n",
      "Epoch: [3/5], Step: [501/3125], Validation Acc: 68.7\n",
      "Epoch: [3/5], Step: [501/3125], Training Loss: 0.6875826120376587\n",
      "Epoch: [3/5], Step: [601/3125], Validation Acc: 69.5\n",
      "Epoch: [3/5], Step: [601/3125], Training Loss: 0.5724852681159973\n",
      "Epoch: [3/5], Step: [701/3125], Validation Acc: 69.9\n",
      "Epoch: [3/5], Step: [701/3125], Training Loss: 0.6085535883903503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3/5], Step: [801/3125], Validation Acc: 66.9\n",
      "Epoch: [3/5], Step: [801/3125], Training Loss: 0.650118887424469\n",
      "Epoch: [3/5], Step: [901/3125], Validation Acc: 69.2\n",
      "Epoch: [3/5], Step: [901/3125], Training Loss: 0.612956166267395\n",
      "Epoch: [3/5], Step: [1001/3125], Validation Acc: 69.1\n",
      "Epoch: [3/5], Step: [1001/3125], Training Loss: 0.7368797659873962\n",
      "Epoch: [3/5], Step: [1101/3125], Validation Acc: 69.6\n",
      "Epoch: [3/5], Step: [1101/3125], Training Loss: 0.5010706186294556\n",
      "Epoch: [3/5], Step: [1201/3125], Validation Acc: 71.2\n",
      "Epoch: [3/5], Step: [1201/3125], Training Loss: 0.6585914492607117\n",
      "Epoch: [3/5], Step: [1301/3125], Validation Acc: 70.4\n",
      "Epoch: [3/5], Step: [1301/3125], Training Loss: 0.6809333562850952\n",
      "Epoch: [3/5], Step: [1401/3125], Validation Acc: 70.6\n",
      "Epoch: [3/5], Step: [1401/3125], Training Loss: 0.5426694750785828\n",
      "Epoch: [3/5], Step: [1501/3125], Validation Acc: 69.2\n",
      "Epoch: [3/5], Step: [1501/3125], Training Loss: 0.6541162729263306\n",
      "Epoch: [3/5], Step: [1601/3125], Validation Acc: 69.8\n",
      "Epoch: [3/5], Step: [1601/3125], Training Loss: 0.6669322848320007\n",
      "Epoch: [3/5], Step: [1701/3125], Validation Acc: 69.5\n",
      "Epoch: [3/5], Step: [1701/3125], Training Loss: 0.6146249771118164\n",
      "Epoch: [3/5], Step: [1801/3125], Validation Acc: 70.4\n",
      "Epoch: [3/5], Step: [1801/3125], Training Loss: 0.4834996163845062\n",
      "Epoch: [3/5], Step: [1901/3125], Validation Acc: 70.2\n",
      "Epoch: [3/5], Step: [1901/3125], Training Loss: 0.6099361181259155\n",
      "Epoch: [3/5], Step: [2001/3125], Validation Acc: 70.1\n",
      "Epoch: [3/5], Step: [2001/3125], Training Loss: 0.7199766635894775\n",
      "Epoch: [3/5], Step: [2101/3125], Validation Acc: 72.7\n",
      "Epoch: [3/5], Step: [2101/3125], Training Loss: 0.7394532561302185\n",
      "Epoch: [3/5], Step: [2201/3125], Validation Acc: 71.1\n",
      "Epoch: [3/5], Step: [2201/3125], Training Loss: 0.6880029439926147\n",
      "Epoch: [3/5], Step: [2301/3125], Validation Acc: 71.2\n",
      "Epoch: [3/5], Step: [2301/3125], Training Loss: 0.7393432855606079\n",
      "Epoch: [3/5], Step: [2401/3125], Validation Acc: 70.8\n",
      "Epoch: [3/5], Step: [2401/3125], Training Loss: 0.3955874443054199\n",
      "Epoch: [3/5], Step: [2501/3125], Validation Acc: 70.3\n",
      "Epoch: [3/5], Step: [2501/3125], Training Loss: 0.8488265872001648\n",
      "Epoch: [3/5], Step: [2601/3125], Validation Acc: 70.7\n",
      "Epoch: [3/5], Step: [2601/3125], Training Loss: 0.8091516494750977\n",
      "Epoch: [3/5], Step: [2701/3125], Validation Acc: 70.1\n",
      "Epoch: [3/5], Step: [2701/3125], Training Loss: 0.7852866053581238\n",
      "Epoch: [3/5], Step: [2801/3125], Validation Acc: 69.9\n",
      "Epoch: [3/5], Step: [2801/3125], Training Loss: 0.5869776606559753\n",
      "Epoch: [3/5], Step: [2901/3125], Validation Acc: 71.6\n",
      "Epoch: [3/5], Step: [2901/3125], Training Loss: 0.653430700302124\n",
      "Epoch: [3/5], Step: [3001/3125], Validation Acc: 71.2\n",
      "Epoch: [3/5], Step: [3001/3125], Training Loss: 0.6766602396965027\n",
      "Epoch: [3/5], Step: [3101/3125], Validation Acc: 70.8\n",
      "Epoch: [3/5], Step: [3101/3125], Training Loss: 0.6863635778427124\n",
      "Epoch: [4/5], Step: [101/3125], Validation Acc: 70.9\n",
      "Epoch: [4/5], Step: [101/3125], Training Loss: 0.6125301718711853\n",
      "Epoch: [4/5], Step: [201/3125], Validation Acc: 69.8\n",
      "Epoch: [4/5], Step: [201/3125], Training Loss: 0.5636339783668518\n",
      "Epoch: [4/5], Step: [301/3125], Validation Acc: 70.4\n",
      "Epoch: [4/5], Step: [301/3125], Training Loss: 0.5802949666976929\n",
      "Epoch: [4/5], Step: [401/3125], Validation Acc: 70.2\n",
      "Epoch: [4/5], Step: [401/3125], Training Loss: 0.5937666296958923\n",
      "Epoch: [4/5], Step: [501/3125], Validation Acc: 70.8\n",
      "Epoch: [4/5], Step: [501/3125], Training Loss: 0.6646944284439087\n",
      "Epoch: [4/5], Step: [601/3125], Validation Acc: 70.5\n",
      "Epoch: [4/5], Step: [601/3125], Training Loss: 0.37066739797592163\n",
      "Epoch: [4/5], Step: [701/3125], Validation Acc: 70.8\n",
      "Epoch: [4/5], Step: [701/3125], Training Loss: 0.45887765288352966\n",
      "Epoch: [4/5], Step: [801/3125], Validation Acc: 71.0\n",
      "Epoch: [4/5], Step: [801/3125], Training Loss: 0.4656848609447479\n",
      "Epoch: [4/5], Step: [901/3125], Validation Acc: 71.4\n",
      "Epoch: [4/5], Step: [901/3125], Training Loss: 0.5644480586051941\n",
      "Epoch: [4/5], Step: [1001/3125], Validation Acc: 71.6\n",
      "Epoch: [4/5], Step: [1001/3125], Training Loss: 0.7645581364631653\n",
      "Epoch: [4/5], Step: [1101/3125], Validation Acc: 72.1\n",
      "Epoch: [4/5], Step: [1101/3125], Training Loss: 0.4653147757053375\n",
      "Epoch: [4/5], Step: [1201/3125], Validation Acc: 71.2\n",
      "Epoch: [4/5], Step: [1201/3125], Training Loss: 0.47337499260902405\n",
      "Epoch: [4/5], Step: [1301/3125], Validation Acc: 70.9\n",
      "Epoch: [4/5], Step: [1301/3125], Training Loss: 0.634924054145813\n",
      "Epoch: [4/5], Step: [1401/3125], Validation Acc: 70.7\n",
      "Epoch: [4/5], Step: [1401/3125], Training Loss: 0.7772984504699707\n",
      "Epoch: [4/5], Step: [1501/3125], Validation Acc: 71.9\n",
      "Epoch: [4/5], Step: [1501/3125], Training Loss: 0.57129967212677\n",
      "Epoch: [4/5], Step: [1601/3125], Validation Acc: 71.3\n",
      "Epoch: [4/5], Step: [1601/3125], Training Loss: 0.5797553658485413\n",
      "Epoch: [4/5], Step: [1701/3125], Validation Acc: 70.9\n",
      "Epoch: [4/5], Step: [1701/3125], Training Loss: 0.6386619806289673\n",
      "Epoch: [4/5], Step: [1801/3125], Validation Acc: 70.3\n",
      "Epoch: [4/5], Step: [1801/3125], Training Loss: 0.5892894268035889\n",
      "Epoch: [4/5], Step: [1901/3125], Validation Acc: 71.6\n",
      "Epoch: [4/5], Step: [1901/3125], Training Loss: 0.7917914390563965\n",
      "Epoch: [4/5], Step: [2001/3125], Validation Acc: 72.0\n",
      "Epoch: [4/5], Step: [2001/3125], Training Loss: 0.6285629272460938\n",
      "Epoch: [4/5], Step: [2101/3125], Validation Acc: 71.0\n",
      "Epoch: [4/5], Step: [2101/3125], Training Loss: 0.5534399151802063\n",
      "Epoch: [4/5], Step: [2201/3125], Validation Acc: 71.0\n",
      "Epoch: [4/5], Step: [2201/3125], Training Loss: 0.5274338722229004\n",
      "Epoch: [4/5], Step: [2301/3125], Validation Acc: 71.0\n",
      "Epoch: [4/5], Step: [2301/3125], Training Loss: 0.5736510157585144\n",
      "Epoch: [4/5], Step: [2401/3125], Validation Acc: 71.5\n",
      "Epoch: [4/5], Step: [2401/3125], Training Loss: 0.7183356285095215\n",
      "Epoch: [4/5], Step: [2501/3125], Validation Acc: 72.3\n",
      "Epoch: [4/5], Step: [2501/3125], Training Loss: 0.5669750571250916\n",
      "Epoch: [4/5], Step: [2601/3125], Validation Acc: 69.4\n",
      "Epoch: [4/5], Step: [2601/3125], Training Loss: 0.4806786775588989\n",
      "Epoch: [4/5], Step: [2701/3125], Validation Acc: 71.6\n",
      "Epoch: [4/5], Step: [2701/3125], Training Loss: 0.6191892027854919\n",
      "Epoch: [4/5], Step: [2801/3125], Validation Acc: 71.5\n",
      "Epoch: [4/5], Step: [2801/3125], Training Loss: 0.5021324753761292\n",
      "Epoch: [4/5], Step: [2901/3125], Validation Acc: 71.3\n",
      "Epoch: [4/5], Step: [2901/3125], Training Loss: 0.5337055921554565\n",
      "Epoch: [4/5], Step: [3001/3125], Validation Acc: 70.5\n",
      "Epoch: [4/5], Step: [3001/3125], Training Loss: 0.6361814737319946\n",
      "Epoch: [4/5], Step: [3101/3125], Validation Acc: 69.6\n",
      "Epoch: [4/5], Step: [3101/3125], Training Loss: 0.5138819813728333\n",
      "Epoch: [5/5], Step: [101/3125], Validation Acc: 70.4\n",
      "Epoch: [5/5], Step: [101/3125], Training Loss: 0.3959043025970459\n",
      "Epoch: [5/5], Step: [201/3125], Validation Acc: 70.6\n",
      "Epoch: [5/5], Step: [201/3125], Training Loss: 0.6294724941253662\n",
      "Epoch: [5/5], Step: [301/3125], Validation Acc: 71.2\n",
      "Epoch: [5/5], Step: [301/3125], Training Loss: 0.717483401298523\n",
      "Epoch: [5/5], Step: [401/3125], Validation Acc: 70.8\n",
      "Epoch: [5/5], Step: [401/3125], Training Loss: 0.4698805510997772\n",
      "Epoch: [5/5], Step: [501/3125], Validation Acc: 70.5\n",
      "Epoch: [5/5], Step: [501/3125], Training Loss: 0.8415710926055908\n",
      "Epoch: [5/5], Step: [601/3125], Validation Acc: 71.2\n",
      "Epoch: [5/5], Step: [601/3125], Training Loss: 0.4411679804325104\n",
      "Epoch: [5/5], Step: [701/3125], Validation Acc: 70.8\n",
      "Epoch: [5/5], Step: [701/3125], Training Loss: 0.39623335003852844\n",
      "Epoch: [5/5], Step: [801/3125], Validation Acc: 72.1\n",
      "Epoch: [5/5], Step: [801/3125], Training Loss: 0.6698306798934937\n",
      "Epoch: [5/5], Step: [901/3125], Validation Acc: 72.1\n",
      "Epoch: [5/5], Step: [901/3125], Training Loss: 0.5834115743637085\n",
      "Epoch: [5/5], Step: [1001/3125], Validation Acc: 71.6\n",
      "Epoch: [5/5], Step: [1001/3125], Training Loss: 0.4583989381790161\n",
      "Epoch: [5/5], Step: [1101/3125], Validation Acc: 69.5\n",
      "Epoch: [5/5], Step: [1101/3125], Training Loss: 0.5312790274620056\n",
      "Epoch: [5/5], Step: [1201/3125], Validation Acc: 68.2\n",
      "Epoch: [5/5], Step: [1201/3125], Training Loss: 0.6120601892471313\n",
      "Epoch: [5/5], Step: [1301/3125], Validation Acc: 70.7\n",
      "Epoch: [5/5], Step: [1301/3125], Training Loss: 0.5161184072494507\n",
      "Epoch: [5/5], Step: [1401/3125], Validation Acc: 71.9\n",
      "Epoch: [5/5], Step: [1401/3125], Training Loss: 0.6906119585037231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5/5], Step: [1501/3125], Validation Acc: 71.8\n",
      "Epoch: [5/5], Step: [1501/3125], Training Loss: 0.6359270215034485\n",
      "Epoch: [5/5], Step: [1601/3125], Validation Acc: 72.0\n",
      "Epoch: [5/5], Step: [1601/3125], Training Loss: 0.5624775290489197\n",
      "Epoch: [5/5], Step: [1701/3125], Validation Acc: 72.1\n",
      "Epoch: [5/5], Step: [1701/3125], Training Loss: 0.3539153039455414\n",
      "Epoch: [5/5], Step: [1801/3125], Validation Acc: 71.7\n",
      "Epoch: [5/5], Step: [1801/3125], Training Loss: 0.4758181571960449\n",
      "Epoch: [5/5], Step: [1901/3125], Validation Acc: 71.2\n",
      "Epoch: [5/5], Step: [1901/3125], Training Loss: 0.45438140630722046\n",
      "Epoch: [5/5], Step: [2001/3125], Validation Acc: 71.6\n",
      "Epoch: [5/5], Step: [2001/3125], Training Loss: 0.46809372305870056\n",
      "Epoch: [5/5], Step: [2101/3125], Validation Acc: 71.7\n",
      "Epoch: [5/5], Step: [2101/3125], Training Loss: 0.5030506253242493\n",
      "Epoch: [5/5], Step: [2201/3125], Validation Acc: 72.5\n",
      "Epoch: [5/5], Step: [2201/3125], Training Loss: 0.7975351810455322\n",
      "Epoch: [5/5], Step: [2301/3125], Validation Acc: 73.3\n",
      "Epoch: [5/5], Step: [2301/3125], Training Loss: 0.2956885099411011\n",
      "Epoch: [5/5], Step: [2401/3125], Validation Acc: 71.9\n",
      "Epoch: [5/5], Step: [2401/3125], Training Loss: 0.43893393874168396\n",
      "Epoch: [5/5], Step: [2501/3125], Validation Acc: 72.8\n",
      "Epoch: [5/5], Step: [2501/3125], Training Loss: 0.5695385932922363\n",
      "Epoch: [5/5], Step: [2601/3125], Validation Acc: 71.3\n",
      "Epoch: [5/5], Step: [2601/3125], Training Loss: 0.470662921667099\n",
      "Epoch: [5/5], Step: [2701/3125], Validation Acc: 73.1\n",
      "Epoch: [5/5], Step: [2701/3125], Training Loss: 0.4428393542766571\n",
      "Epoch: [5/5], Step: [2801/3125], Validation Acc: 72.6\n",
      "Epoch: [5/5], Step: [2801/3125], Training Loss: 0.6939597725868225\n",
      "Epoch: [5/5], Step: [2901/3125], Validation Acc: 72.1\n",
      "Epoch: [5/5], Step: [2901/3125], Training Loss: 0.5085858106613159\n",
      "Epoch: [5/5], Step: [3001/3125], Validation Acc: 73.3\n",
      "Epoch: [5/5], Step: [3001/3125], Training Loss: 0.7229784727096558\n",
      "Epoch: [5/5], Step: [3101/3125], Validation Acc: 72.9\n",
      "Epoch: [5/5], Step: [3101/3125], Training Loss: 0.5009821653366089\n",
      "Max Validation Accuracy: 73.3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "73.3"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = rnn(emb_size = 300, hidden_size=100, num_layers=1, num_classes=3) \n",
    "train_model(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnn(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, num_layers, num_classes):\n",
    "\n",
    "        super(cnn, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(WEIGHTS), freeze=True)\n",
    "    \n",
    "        self.conv1 = nn.Conv1d(emb_size, hidden_size, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding=1)\n",
    "        self.maxpool = nn.MaxPool1d(60)\n",
    "        self.linear1 = nn.Linear(2*hidden_size, 500)\n",
    "        self.linear2 = nn.Linear(500, num_classes)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, x, sent1_lengths, sent2_lengths):\n",
    "        \n",
    "        batch_size = x.size()[0]\n",
    "        seq_len = x.size()[2]\n",
    "        \n",
    "        sent1s = torch.tensor(x[:, 0, :])\n",
    "        sent2s = torch.tensor(x[:, 1, :])\n",
    "        ordered_sents = torch.cat([sent1s, sent2s], dim=0)\n",
    "\n",
    "        embed = self.embedding(ordered_sents)\n",
    "        hidden = self.conv1(embed.transpose(1,2)).transpose(1,2)\n",
    "        hidden = F.relu(hidden.contiguous().view(-1, hidden.size(-1))).view(2*batch_size, seq_len, hidden.size(-1))\n",
    "\n",
    "        hidden = self.conv2(hidden.transpose(1,2)).transpose(1,2)\n",
    "        hidden = F.relu(hidden.contiguous().view(-1, hidden.size(-1))).view(2*batch_size, seq_len, hidden.size(-1))\n",
    "        hidden = self.maxpool(hidden.transpose(1, 2)).transpose(1, 2).squeeze(dim=1)\n",
    "        \n",
    "        hidden_sent1s = hidden[0:batch_size, :]\n",
    "        hidden_sent2s = hidden[batch_size:, :]     \n",
    "        \n",
    "        linear1 = self.linear1(torch.cat([hidden_sent1s, hidden_sent2s], dim=1))\n",
    "        #linear1 = self.linear1(torch.tensor(hidden_sent1s) + torch.tensor(hidden_sent2s))\n",
    "        #linear1 = self.linear1(torch.tensor(hidden_sent1s)*torch.tensor(hidden_sent2s))\n",
    "        linear1 = F.relu(linear1.contiguous().view(-1, linear1.size(-1))).view(linear1.shape)\n",
    "        linear1 = self.dropout(linear1)\n",
    "        logits = self.linear2(linear1)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/5], Step: [101/3125], Validation Acc: 43.8\n",
      "Epoch: [1/5], Step: [101/3125], Training Loss: 1.1325687170028687\n",
      "Epoch: [1/5], Step: [201/3125], Validation Acc: 51.5\n",
      "Epoch: [1/5], Step: [201/3125], Training Loss: 0.9913889765739441\n",
      "Epoch: [1/5], Step: [301/3125], Validation Acc: 57.6\n",
      "Epoch: [1/5], Step: [301/3125], Training Loss: 0.9026820659637451\n",
      "Epoch: [1/5], Step: [401/3125], Validation Acc: 58.3\n",
      "Epoch: [1/5], Step: [401/3125], Training Loss: 0.9187735319137573\n",
      "Epoch: [1/5], Step: [501/3125], Validation Acc: 58.0\n",
      "Epoch: [1/5], Step: [501/3125], Training Loss: 0.9364725351333618\n",
      "Epoch: [1/5], Step: [601/3125], Validation Acc: 59.6\n",
      "Epoch: [1/5], Step: [601/3125], Training Loss: 0.9069799184799194\n",
      "Epoch: [1/5], Step: [701/3125], Validation Acc: 59.7\n",
      "Epoch: [1/5], Step: [701/3125], Training Loss: 0.7501728534698486\n",
      "Epoch: [1/5], Step: [801/3125], Validation Acc: 60.6\n",
      "Epoch: [1/5], Step: [801/3125], Training Loss: 0.8851989507675171\n",
      "Epoch: [1/5], Step: [901/3125], Validation Acc: 59.9\n",
      "Epoch: [1/5], Step: [901/3125], Training Loss: 0.9167454242706299\n",
      "Epoch: [1/5], Step: [1001/3125], Validation Acc: 62.4\n",
      "Epoch: [1/5], Step: [1001/3125], Training Loss: 0.7949150204658508\n",
      "Epoch: [1/5], Step: [1101/3125], Validation Acc: 60.3\n",
      "Epoch: [1/5], Step: [1101/3125], Training Loss: 0.8015872836112976\n",
      "Epoch: [1/5], Step: [1201/3125], Validation Acc: 61.0\n",
      "Epoch: [1/5], Step: [1201/3125], Training Loss: 0.8124764561653137\n",
      "Epoch: [1/5], Step: [1301/3125], Validation Acc: 61.8\n",
      "Epoch: [1/5], Step: [1301/3125], Training Loss: 0.8880605101585388\n",
      "Epoch: [1/5], Step: [1401/3125], Validation Acc: 61.8\n",
      "Epoch: [1/5], Step: [1401/3125], Training Loss: 0.8318989276885986\n",
      "Epoch: [1/5], Step: [1501/3125], Validation Acc: 62.7\n",
      "Epoch: [1/5], Step: [1501/3125], Training Loss: 0.7000021934509277\n",
      "Epoch: [1/5], Step: [1601/3125], Validation Acc: 62.7\n",
      "Epoch: [1/5], Step: [1601/3125], Training Loss: 0.9062539935112\n",
      "Epoch: [1/5], Step: [1701/3125], Validation Acc: 62.4\n",
      "Epoch: [1/5], Step: [1701/3125], Training Loss: 0.8224246501922607\n",
      "Epoch: [1/5], Step: [1801/3125], Validation Acc: 61.7\n",
      "Epoch: [1/5], Step: [1801/3125], Training Loss: 0.7547543048858643\n",
      "Epoch: [1/5], Step: [1901/3125], Validation Acc: 64.1\n",
      "Epoch: [1/5], Step: [1901/3125], Training Loss: 0.7843115329742432\n",
      "Epoch: [1/5], Step: [2001/3125], Validation Acc: 63.0\n",
      "Epoch: [1/5], Step: [2001/3125], Training Loss: 0.6687904000282288\n",
      "Epoch: [1/5], Step: [2101/3125], Validation Acc: 60.7\n",
      "Epoch: [1/5], Step: [2101/3125], Training Loss: 0.8098275065422058\n",
      "Epoch: [1/5], Step: [2201/3125], Validation Acc: 62.1\n",
      "Epoch: [1/5], Step: [2201/3125], Training Loss: 0.7664301991462708\n",
      "Epoch: [1/5], Step: [2301/3125], Validation Acc: 63.7\n",
      "Epoch: [1/5], Step: [2301/3125], Training Loss: 0.7422773241996765\n",
      "Epoch: [1/5], Step: [2401/3125], Validation Acc: 64.5\n",
      "Epoch: [1/5], Step: [2401/3125], Training Loss: 0.6770035624504089\n",
      "Epoch: [1/5], Step: [2501/3125], Validation Acc: 63.0\n",
      "Epoch: [1/5], Step: [2501/3125], Training Loss: 0.962485671043396\n",
      "Epoch: [1/5], Step: [2601/3125], Validation Acc: 64.1\n",
      "Epoch: [1/5], Step: [2601/3125], Training Loss: 1.0193843841552734\n",
      "Epoch: [1/5], Step: [2701/3125], Validation Acc: 62.4\n",
      "Epoch: [1/5], Step: [2701/3125], Training Loss: 0.7727336287498474\n",
      "Epoch: [1/5], Step: [2801/3125], Validation Acc: 64.4\n",
      "Epoch: [1/5], Step: [2801/3125], Training Loss: 0.7080672383308411\n",
      "Epoch: [1/5], Step: [2901/3125], Validation Acc: 64.2\n",
      "Epoch: [1/5], Step: [2901/3125], Training Loss: 0.9590474367141724\n",
      "Epoch: [1/5], Step: [3001/3125], Validation Acc: 63.3\n",
      "Epoch: [1/5], Step: [3001/3125], Training Loss: 0.5749902725219727\n",
      "Epoch: [1/5], Step: [3101/3125], Validation Acc: 62.7\n",
      "Epoch: [1/5], Step: [3101/3125], Training Loss: 0.8561968803405762\n",
      "Epoch: [2/5], Step: [101/3125], Validation Acc: 62.8\n",
      "Epoch: [2/5], Step: [101/3125], Training Loss: 0.5940475463867188\n",
      "Epoch: [2/5], Step: [201/3125], Validation Acc: 63.9\n",
      "Epoch: [2/5], Step: [201/3125], Training Loss: 0.7720518708229065\n",
      "Epoch: [2/5], Step: [301/3125], Validation Acc: 62.5\n",
      "Epoch: [2/5], Step: [301/3125], Training Loss: 0.931773841381073\n",
      "Epoch: [2/5], Step: [401/3125], Validation Acc: 63.6\n",
      "Epoch: [2/5], Step: [401/3125], Training Loss: 0.7392380237579346\n",
      "Epoch: [2/5], Step: [501/3125], Validation Acc: 65.5\n",
      "Epoch: [2/5], Step: [501/3125], Training Loss: 0.8125961422920227\n",
      "Epoch: [2/5], Step: [601/3125], Validation Acc: 63.0\n",
      "Epoch: [2/5], Step: [601/3125], Training Loss: 0.7088313102722168\n",
      "Epoch: [2/5], Step: [701/3125], Validation Acc: 65.0\n",
      "Epoch: [2/5], Step: [701/3125], Training Loss: 0.6610659956932068\n",
      "Epoch: [2/5], Step: [801/3125], Validation Acc: 65.8\n",
      "Epoch: [2/5], Step: [801/3125], Training Loss: 0.6449522972106934\n",
      "Epoch: [2/5], Step: [901/3125], Validation Acc: 65.5\n",
      "Epoch: [2/5], Step: [901/3125], Training Loss: 0.7766550183296204\n",
      "Epoch: [2/5], Step: [1001/3125], Validation Acc: 65.4\n",
      "Epoch: [2/5], Step: [1001/3125], Training Loss: 0.7157125473022461\n",
      "Epoch: [2/5], Step: [1101/3125], Validation Acc: 65.3\n",
      "Epoch: [2/5], Step: [1101/3125], Training Loss: 0.7731817364692688\n",
      "Epoch: [2/5], Step: [1201/3125], Validation Acc: 65.9\n",
      "Epoch: [2/5], Step: [1201/3125], Training Loss: 0.6700427532196045\n",
      "Epoch: [2/5], Step: [1301/3125], Validation Acc: 66.8\n",
      "Epoch: [2/5], Step: [1301/3125], Training Loss: 0.8111910820007324\n",
      "Epoch: [2/5], Step: [1401/3125], Validation Acc: 66.1\n",
      "Epoch: [2/5], Step: [1401/3125], Training Loss: 0.7561468482017517\n",
      "Epoch: [2/5], Step: [1501/3125], Validation Acc: 65.6\n",
      "Epoch: [2/5], Step: [1501/3125], Training Loss: 0.8250732421875\n",
      "Epoch: [2/5], Step: [1601/3125], Validation Acc: 64.5\n",
      "Epoch: [2/5], Step: [1601/3125], Training Loss: 0.7046797275543213\n",
      "Epoch: [2/5], Step: [1701/3125], Validation Acc: 66.6\n",
      "Epoch: [2/5], Step: [1701/3125], Training Loss: 0.844980776309967\n",
      "Epoch: [2/5], Step: [1801/3125], Validation Acc: 65.8\n",
      "Epoch: [2/5], Step: [1801/3125], Training Loss: 0.663061797618866\n",
      "Epoch: [2/5], Step: [1901/3125], Validation Acc: 64.3\n",
      "Epoch: [2/5], Step: [1901/3125], Training Loss: 0.6382068991661072\n",
      "Epoch: [2/5], Step: [2001/3125], Validation Acc: 66.9\n",
      "Epoch: [2/5], Step: [2001/3125], Training Loss: 0.9047402739524841\n",
      "Epoch: [2/5], Step: [2101/3125], Validation Acc: 66.4\n",
      "Epoch: [2/5], Step: [2101/3125], Training Loss: 0.684929609298706\n",
      "Epoch: [2/5], Step: [2201/3125], Validation Acc: 67.0\n",
      "Epoch: [2/5], Step: [2201/3125], Training Loss: 0.6993873119354248\n",
      "Epoch: [2/5], Step: [2301/3125], Validation Acc: 67.0\n",
      "Epoch: [2/5], Step: [2301/3125], Training Loss: 0.5855634212493896\n",
      "Epoch: [2/5], Step: [2401/3125], Validation Acc: 66.9\n",
      "Epoch: [2/5], Step: [2401/3125], Training Loss: 0.8590852618217468\n",
      "Epoch: [2/5], Step: [2501/3125], Validation Acc: 68.9\n",
      "Epoch: [2/5], Step: [2501/3125], Training Loss: 0.7948504686355591\n",
      "Epoch: [2/5], Step: [2601/3125], Validation Acc: 67.2\n",
      "Epoch: [2/5], Step: [2601/3125], Training Loss: 0.7102537751197815\n",
      "Epoch: [2/5], Step: [2701/3125], Validation Acc: 66.6\n",
      "Epoch: [2/5], Step: [2701/3125], Training Loss: 0.7479597926139832\n",
      "Epoch: [2/5], Step: [2801/3125], Validation Acc: 68.1\n",
      "Epoch: [2/5], Step: [2801/3125], Training Loss: 0.7860345840454102\n",
      "Epoch: [2/5], Step: [2901/3125], Validation Acc: 69.4\n",
      "Epoch: [2/5], Step: [2901/3125], Training Loss: 0.7985386252403259\n",
      "Epoch: [2/5], Step: [3001/3125], Validation Acc: 67.0\n",
      "Epoch: [2/5], Step: [3001/3125], Training Loss: 0.8460261225700378\n",
      "Epoch: [2/5], Step: [3101/3125], Validation Acc: 66.0\n",
      "Epoch: [2/5], Step: [3101/3125], Training Loss: 0.7819804549217224\n",
      "Epoch: [3/5], Step: [101/3125], Validation Acc: 67.6\n",
      "Epoch: [3/5], Step: [101/3125], Training Loss: 0.8860428929328918\n",
      "Epoch: [3/5], Step: [201/3125], Validation Acc: 65.7\n",
      "Epoch: [3/5], Step: [201/3125], Training Loss: 0.6559000015258789\n",
      "Epoch: [3/5], Step: [301/3125], Validation Acc: 65.9\n",
      "Epoch: [3/5], Step: [301/3125], Training Loss: 0.4827231168746948\n",
      "Epoch: [3/5], Step: [401/3125], Validation Acc: 67.1\n",
      "Epoch: [3/5], Step: [401/3125], Training Loss: 0.558433473110199\n",
      "Epoch: [3/5], Step: [501/3125], Validation Acc: 67.5\n",
      "Epoch: [3/5], Step: [501/3125], Training Loss: 1.046957015991211\n",
      "Epoch: [3/5], Step: [601/3125], Validation Acc: 67.8\n",
      "Epoch: [3/5], Step: [601/3125], Training Loss: 0.6475303173065186\n",
      "Epoch: [3/5], Step: [701/3125], Validation Acc: 66.9\n",
      "Epoch: [3/5], Step: [701/3125], Training Loss: 0.7378974556922913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3/5], Step: [801/3125], Validation Acc: 66.7\n",
      "Epoch: [3/5], Step: [801/3125], Training Loss: 0.763251006603241\n",
      "Epoch: [3/5], Step: [901/3125], Validation Acc: 67.6\n",
      "Epoch: [3/5], Step: [901/3125], Training Loss: 0.7733426690101624\n",
      "Epoch: [3/5], Step: [1001/3125], Validation Acc: 67.9\n",
      "Epoch: [3/5], Step: [1001/3125], Training Loss: 0.7706139087677002\n",
      "Epoch: [3/5], Step: [1101/3125], Validation Acc: 67.0\n",
      "Epoch: [3/5], Step: [1101/3125], Training Loss: 0.925399661064148\n",
      "Epoch: [3/5], Step: [1201/3125], Validation Acc: 67.8\n",
      "Epoch: [3/5], Step: [1201/3125], Training Loss: 0.7828293442726135\n",
      "Epoch: [3/5], Step: [1301/3125], Validation Acc: 68.0\n",
      "Epoch: [3/5], Step: [1301/3125], Training Loss: 0.7847010493278503\n",
      "Epoch: [3/5], Step: [1401/3125], Validation Acc: 67.8\n",
      "Epoch: [3/5], Step: [1401/3125], Training Loss: 0.6668834686279297\n",
      "Epoch: [3/5], Step: [1501/3125], Validation Acc: 66.8\n",
      "Epoch: [3/5], Step: [1501/3125], Training Loss: 0.6030179262161255\n",
      "Epoch: [3/5], Step: [1601/3125], Validation Acc: 67.5\n",
      "Epoch: [3/5], Step: [1601/3125], Training Loss: 0.7091391086578369\n",
      "Epoch: [3/5], Step: [1701/3125], Validation Acc: 67.5\n",
      "Epoch: [3/5], Step: [1701/3125], Training Loss: 0.5438401103019714\n",
      "Epoch: [3/5], Step: [1801/3125], Validation Acc: 67.6\n",
      "Epoch: [3/5], Step: [1801/3125], Training Loss: 0.7387388944625854\n",
      "Epoch: [3/5], Step: [1901/3125], Validation Acc: 67.8\n",
      "Epoch: [3/5], Step: [1901/3125], Training Loss: 0.6844256520271301\n",
      "Epoch: [3/5], Step: [2001/3125], Validation Acc: 67.2\n",
      "Epoch: [3/5], Step: [2001/3125], Training Loss: 0.5672680735588074\n",
      "Epoch: [3/5], Step: [2101/3125], Validation Acc: 68.0\n",
      "Epoch: [3/5], Step: [2101/3125], Training Loss: 0.7096994519233704\n",
      "Epoch: [3/5], Step: [2201/3125], Validation Acc: 68.5\n",
      "Epoch: [3/5], Step: [2201/3125], Training Loss: 0.9339829683303833\n",
      "Epoch: [3/5], Step: [2301/3125], Validation Acc: 69.1\n",
      "Epoch: [3/5], Step: [2301/3125], Training Loss: 0.6837010979652405\n",
      "Epoch: [3/5], Step: [2401/3125], Validation Acc: 68.1\n",
      "Epoch: [3/5], Step: [2401/3125], Training Loss: 0.7780656218528748\n",
      "Epoch: [3/5], Step: [2501/3125], Validation Acc: 69.7\n",
      "Epoch: [3/5], Step: [2501/3125], Training Loss: 0.787855863571167\n",
      "Epoch: [3/5], Step: [2601/3125], Validation Acc: 69.3\n",
      "Epoch: [3/5], Step: [2601/3125], Training Loss: 0.6835556626319885\n",
      "Epoch: [3/5], Step: [2701/3125], Validation Acc: 69.0\n",
      "Epoch: [3/5], Step: [2701/3125], Training Loss: 0.6877894997596741\n",
      "Epoch: [3/5], Step: [2801/3125], Validation Acc: 68.6\n",
      "Epoch: [3/5], Step: [2801/3125], Training Loss: 0.7205697298049927\n",
      "Epoch: [3/5], Step: [2901/3125], Validation Acc: 67.8\n",
      "Epoch: [3/5], Step: [2901/3125], Training Loss: 0.531602680683136\n",
      "Epoch: [3/5], Step: [3001/3125], Validation Acc: 68.2\n",
      "Epoch: [3/5], Step: [3001/3125], Training Loss: 0.7326458096504211\n",
      "Epoch: [3/5], Step: [3101/3125], Validation Acc: 68.7\n",
      "Epoch: [3/5], Step: [3101/3125], Training Loss: 0.7985178828239441\n",
      "Epoch: [4/5], Step: [101/3125], Validation Acc: 69.4\n",
      "Epoch: [4/5], Step: [101/3125], Training Loss: 0.6193839907646179\n",
      "Epoch: [4/5], Step: [201/3125], Validation Acc: 68.2\n",
      "Epoch: [4/5], Step: [201/3125], Training Loss: 0.5607810020446777\n",
      "Epoch: [4/5], Step: [301/3125], Validation Acc: 68.1\n",
      "Epoch: [4/5], Step: [301/3125], Training Loss: 0.6359155774116516\n",
      "Epoch: [4/5], Step: [401/3125], Validation Acc: 67.7\n",
      "Epoch: [4/5], Step: [401/3125], Training Loss: 0.576047956943512\n",
      "Epoch: [4/5], Step: [501/3125], Validation Acc: 67.1\n",
      "Epoch: [4/5], Step: [501/3125], Training Loss: 0.5816996097564697\n",
      "Epoch: [4/5], Step: [601/3125], Validation Acc: 68.4\n",
      "Epoch: [4/5], Step: [601/3125], Training Loss: 0.6715265512466431\n",
      "Epoch: [4/5], Step: [701/3125], Validation Acc: 68.3\n",
      "Epoch: [4/5], Step: [701/3125], Training Loss: 0.6623708009719849\n",
      "Epoch: [4/5], Step: [801/3125], Validation Acc: 68.9\n",
      "Epoch: [4/5], Step: [801/3125], Training Loss: 0.45692744851112366\n",
      "Epoch: [4/5], Step: [901/3125], Validation Acc: 68.1\n",
      "Epoch: [4/5], Step: [901/3125], Training Loss: 0.7101200819015503\n",
      "Epoch: [4/5], Step: [1001/3125], Validation Acc: 68.7\n",
      "Epoch: [4/5], Step: [1001/3125], Training Loss: 0.69818514585495\n",
      "Epoch: [4/5], Step: [1101/3125], Validation Acc: 68.9\n",
      "Epoch: [4/5], Step: [1101/3125], Training Loss: 0.7127206325531006\n",
      "Epoch: [4/5], Step: [1201/3125], Validation Acc: 67.4\n",
      "Epoch: [4/5], Step: [1201/3125], Training Loss: 0.671658992767334\n",
      "Epoch: [4/5], Step: [1301/3125], Validation Acc: 68.7\n",
      "Epoch: [4/5], Step: [1301/3125], Training Loss: 0.6616755127906799\n",
      "Epoch: [4/5], Step: [1401/3125], Validation Acc: 68.4\n",
      "Epoch: [4/5], Step: [1401/3125], Training Loss: 0.5886048674583435\n",
      "Epoch: [4/5], Step: [1501/3125], Validation Acc: 68.8\n",
      "Epoch: [4/5], Step: [1501/3125], Training Loss: 0.8542632460594177\n",
      "Epoch: [4/5], Step: [1601/3125], Validation Acc: 68.1\n",
      "Epoch: [4/5], Step: [1601/3125], Training Loss: 0.4663194417953491\n",
      "Epoch: [4/5], Step: [1701/3125], Validation Acc: 67.5\n",
      "Epoch: [4/5], Step: [1701/3125], Training Loss: 0.7176561951637268\n",
      "Epoch: [4/5], Step: [1801/3125], Validation Acc: 69.1\n",
      "Epoch: [4/5], Step: [1801/3125], Training Loss: 0.6599390506744385\n",
      "Epoch: [4/5], Step: [1901/3125], Validation Acc: 69.5\n",
      "Epoch: [4/5], Step: [1901/3125], Training Loss: 0.7192583084106445\n",
      "Epoch: [4/5], Step: [2001/3125], Validation Acc: 68.2\n",
      "Epoch: [4/5], Step: [2001/3125], Training Loss: 0.7926717400550842\n",
      "Epoch: [4/5], Step: [2101/3125], Validation Acc: 67.3\n",
      "Epoch: [4/5], Step: [2101/3125], Training Loss: 0.6983848810195923\n",
      "Epoch: [4/5], Step: [2201/3125], Validation Acc: 69.4\n",
      "Epoch: [4/5], Step: [2201/3125], Training Loss: 0.6303554177284241\n",
      "Epoch: [4/5], Step: [2301/3125], Validation Acc: 68.0\n",
      "Epoch: [4/5], Step: [2301/3125], Training Loss: 0.496198832988739\n",
      "Epoch: [4/5], Step: [2401/3125], Validation Acc: 69.4\n",
      "Epoch: [4/5], Step: [2401/3125], Training Loss: 0.61814945936203\n",
      "Epoch: [4/5], Step: [2501/3125], Validation Acc: 69.2\n",
      "Epoch: [4/5], Step: [2501/3125], Training Loss: 0.7450666427612305\n",
      "Epoch: [4/5], Step: [2601/3125], Validation Acc: 68.2\n",
      "Epoch: [4/5], Step: [2601/3125], Training Loss: 0.7758671045303345\n",
      "Epoch: [4/5], Step: [2701/3125], Validation Acc: 67.8\n",
      "Epoch: [4/5], Step: [2701/3125], Training Loss: 0.6241848468780518\n",
      "Epoch: [4/5], Step: [2801/3125], Validation Acc: 68.9\n",
      "Epoch: [4/5], Step: [2801/3125], Training Loss: 0.8078797459602356\n",
      "Epoch: [4/5], Step: [2901/3125], Validation Acc: 68.7\n",
      "Epoch: [4/5], Step: [2901/3125], Training Loss: 0.9733627438545227\n",
      "Epoch: [4/5], Step: [3001/3125], Validation Acc: 67.1\n",
      "Epoch: [4/5], Step: [3001/3125], Training Loss: 0.44259724020957947\n",
      "Epoch: [4/5], Step: [3101/3125], Validation Acc: 68.9\n",
      "Epoch: [4/5], Step: [3101/3125], Training Loss: 0.5376942157745361\n",
      "Epoch: [5/5], Step: [101/3125], Validation Acc: 68.5\n",
      "Epoch: [5/5], Step: [101/3125], Training Loss: 0.6417964100837708\n",
      "Epoch: [5/5], Step: [201/3125], Validation Acc: 69.6\n",
      "Epoch: [5/5], Step: [201/3125], Training Loss: 0.8356322050094604\n",
      "Epoch: [5/5], Step: [301/3125], Validation Acc: 68.8\n",
      "Epoch: [5/5], Step: [301/3125], Training Loss: 0.8342132568359375\n",
      "Epoch: [5/5], Step: [401/3125], Validation Acc: 69.0\n",
      "Epoch: [5/5], Step: [401/3125], Training Loss: 0.5971733331680298\n",
      "Epoch: [5/5], Step: [501/3125], Validation Acc: 70.0\n",
      "Epoch: [5/5], Step: [501/3125], Training Loss: 0.7694554328918457\n",
      "Epoch: [5/5], Step: [601/3125], Validation Acc: 69.5\n",
      "Epoch: [5/5], Step: [601/3125], Training Loss: 0.5668653249740601\n",
      "Epoch: [5/5], Step: [701/3125], Validation Acc: 69.9\n",
      "Epoch: [5/5], Step: [701/3125], Training Loss: 0.818408727645874\n",
      "Epoch: [5/5], Step: [801/3125], Validation Acc: 69.3\n",
      "Epoch: [5/5], Step: [801/3125], Training Loss: 0.46580690145492554\n",
      "Epoch: [5/5], Step: [901/3125], Validation Acc: 68.6\n",
      "Epoch: [5/5], Step: [901/3125], Training Loss: 0.5372759699821472\n",
      "Epoch: [5/5], Step: [1001/3125], Validation Acc: 69.9\n",
      "Epoch: [5/5], Step: [1001/3125], Training Loss: 0.7736362218856812\n",
      "Epoch: [5/5], Step: [1101/3125], Validation Acc: 70.0\n",
      "Epoch: [5/5], Step: [1101/3125], Training Loss: 0.6668765544891357\n",
      "Epoch: [5/5], Step: [1201/3125], Validation Acc: 70.2\n",
      "Epoch: [5/5], Step: [1201/3125], Training Loss: 0.8321712613105774\n",
      "Epoch: [5/5], Step: [1301/3125], Validation Acc: 70.0\n",
      "Epoch: [5/5], Step: [1301/3125], Training Loss: 0.5228307843208313\n",
      "Epoch: [5/5], Step: [1401/3125], Validation Acc: 68.8\n",
      "Epoch: [5/5], Step: [1401/3125], Training Loss: 0.447488933801651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5/5], Step: [1501/3125], Validation Acc: 71.0\n",
      "Epoch: [5/5], Step: [1501/3125], Training Loss: 0.6698805689811707\n",
      "Epoch: [5/5], Step: [1601/3125], Validation Acc: 69.6\n",
      "Epoch: [5/5], Step: [1601/3125], Training Loss: 0.6566417217254639\n",
      "Epoch: [5/5], Step: [1701/3125], Validation Acc: 70.0\n",
      "Epoch: [5/5], Step: [1701/3125], Training Loss: 0.45077043771743774\n",
      "Epoch: [5/5], Step: [1801/3125], Validation Acc: 70.4\n",
      "Epoch: [5/5], Step: [1801/3125], Training Loss: 0.6122647523880005\n",
      "Epoch: [5/5], Step: [1901/3125], Validation Acc: 70.0\n",
      "Epoch: [5/5], Step: [1901/3125], Training Loss: 0.5797339081764221\n",
      "Epoch: [5/5], Step: [2001/3125], Validation Acc: 70.0\n",
      "Epoch: [5/5], Step: [2001/3125], Training Loss: 0.5525529980659485\n",
      "Epoch: [5/5], Step: [2101/3125], Validation Acc: 68.4\n",
      "Epoch: [5/5], Step: [2101/3125], Training Loss: 0.46992507576942444\n",
      "Epoch: [5/5], Step: [2201/3125], Validation Acc: 69.2\n",
      "Epoch: [5/5], Step: [2201/3125], Training Loss: 0.6059962511062622\n",
      "Epoch: [5/5], Step: [2301/3125], Validation Acc: 69.5\n",
      "Epoch: [5/5], Step: [2301/3125], Training Loss: 0.6996362209320068\n",
      "Epoch: [5/5], Step: [2401/3125], Validation Acc: 69.7\n",
      "Epoch: [5/5], Step: [2401/3125], Training Loss: 0.45001527667045593\n",
      "Epoch: [5/5], Step: [2501/3125], Validation Acc: 68.8\n",
      "Epoch: [5/5], Step: [2501/3125], Training Loss: 0.7571923136711121\n",
      "Epoch: [5/5], Step: [2601/3125], Validation Acc: 69.4\n",
      "Epoch: [5/5], Step: [2601/3125], Training Loss: 0.5002180933952332\n",
      "Epoch: [5/5], Step: [2701/3125], Validation Acc: 69.4\n",
      "Epoch: [5/5], Step: [2701/3125], Training Loss: 0.7453798055648804\n",
      "Epoch: [5/5], Step: [2801/3125], Validation Acc: 67.5\n",
      "Epoch: [5/5], Step: [2801/3125], Training Loss: 0.5534108877182007\n",
      "Epoch: [5/5], Step: [2901/3125], Validation Acc: 68.5\n",
      "Epoch: [5/5], Step: [2901/3125], Training Loss: 0.5169063806533813\n",
      "Epoch: [5/5], Step: [3001/3125], Validation Acc: 69.5\n",
      "Epoch: [5/5], Step: [3001/3125], Training Loss: 0.7256994843482971\n",
      "Epoch: [5/5], Step: [3101/3125], Validation Acc: 69.8\n",
      "Epoch: [5/5], Step: [3101/3125], Training Loss: 0.7946016192436218\n",
      "Max Validation Accuracy: 71.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "71.0"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = cnn(emb_size = 300, hidden_size=50, num_layers=1, num_classes=3)\n",
    "train_model(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Genre NLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_val = pd.read_csv('mnli_val.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['fiction', 'telephone', 'slate', 'government', 'travel'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnli_val['genre'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_val_dict = {}\n",
    "for x in mnli_val['genre'].unique():\n",
    "    filtered = mnli_val[mnli_val['genre'] == x]\n",
    "    mnli_val_dict[x] = {}\n",
    "    mnli_val_dict[x][\"sentence1\"] = list(filtered[\"sentence1\"])\n",
    "    mnli_val_dict[x][\"sentence2\"] = list(filtered[\"sentence2\"])\n",
    "    mnli_val_dict[x][\"label\"] = labels_to_integers(list(filtered[\"label\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genre fiction has validation accuracy: 34.77386934673367\n",
      "Genre telephone has validation accuracy: 36.517412935323385\n",
      "Genre slate has validation accuracy: 34.830339321357286\n",
      "Genre government has validation accuracy: 36.71259842519685\n",
      "Genre travel has validation accuracy: 35.53971486761711\n"
     ]
    }
   ],
   "source": [
    "rnn_results = {}\n",
    "model = rnn(emb_size = 300, hidden_size=100, num_layers=1, num_classes=3)\n",
    "for genre in mnli_val_dict.keys():\n",
    "    \n",
    "    SEED =234\n",
    "    random.Random(SEED).shuffle(mnli_val_dict[genre]['sentence1'])\n",
    "    random.Random(SEED).shuffle(mnli_val_dict[genre]['sentence2'])\n",
    "    random.Random(SEED).shuffle(mnli_val_dict[genre]['label'])\n",
    "    sentence1_tokenized = tokenize(mnli_val_dict[genre]['sentence1'])\n",
    "    sentence2_tokenized = tokenize(mnli_val_dict[genre]['sentence2'])\n",
    "    sentence1_indices = token2index_dataset(sentence1_tokenized)\n",
    "    sentence2_indices = token2index_dataset(sentence2_tokenized)\n",
    "    \n",
    "    dataset = NLIDataset(sentence1_indices, sentence2_indices, mnli_val_dict[genre]['label'])\n",
    "    dataloader = torch.utils.data.DataLoader(dataset=dataset, \n",
    "                                             batch_size=BATCH_SIZE, \n",
    "                                             collate_fn=NLI_collate_func,\n",
    "                                             shuffle=True)\n",
    "    \n",
    "    rnn_results[genre] = test_model(dataloader, model)\n",
    "    print(\"Genre {} has validation accuracy: {}\".format(genre, rnn_results[genre]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genre fiction has validation accuracy: 34.87437185929648\n",
      "Genre telephone has validation accuracy: 36.517412935323385\n",
      "Genre slate has validation accuracy: 34.830339321357286\n",
      "Genre government has validation accuracy: 36.71259842519685\n",
      "Genre travel has validation accuracy: 35.437881873727086\n"
     ]
    }
   ],
   "source": [
    "cnn_results = {}\n",
    "model = cnn(emb_size = 300, hidden_size=50, num_layers=1, num_classes=3)\n",
    "for genre in mnli_val_dict.keys():\n",
    "    \n",
    "    SEED =234\n",
    "    random.Random(SEED).shuffle(mnli_val_dict[genre]['sentence1'])\n",
    "    random.Random(SEED).shuffle(mnli_val_dict[genre]['sentence2'])\n",
    "    random.Random(SEED).shuffle(mnli_val_dict[genre]['label'])\n",
    "    sentence1_tokenized = tokenize(mnli_val_dict[genre]['sentence1'])\n",
    "    sentence2_tokenized = tokenize(mnli_val_dict[genre]['sentence2'])\n",
    "    sentence1_indices = token2index_dataset(sentence1_tokenized)\n",
    "    sentence2_indices = token2index_dataset(sentence2_tokenized)\n",
    "    \n",
    "    dataset = NLIDataset(sentence1_indices, sentence2_indices, mnli_val_dict[genre]['label'])\n",
    "    dataloader = torch.utils.data.DataLoader(dataset=dataset, \n",
    "                                             batch_size=BATCH_SIZE, \n",
    "                                             collate_fn=NLI_collate_func,\n",
    "                                             shuffle=True)\n",
    "    \n",
    "    cnn_results[genre] = test_model(dataloader, model)\n",
    "    print(\"Genre {} has validation accuracy: {}\".format(genre, cnn_results[genre]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
